{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available! You can use GPU acceleration.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA (GPU support) is available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available! You can use GPU acceleration.\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Using CPU instead.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dump model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creation of the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Paul\\.conda\\envs\\GPU\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n"
     ]
    }
   ],
   "source": [
    "import contextlib\n",
    "import io\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "\n",
    "def ids_tensor(shape, vocab_size, rng=None, name=None):\n",
    "    #  Creates a random int32 tensor of the shape within the vocab size\n",
    "    import torch\n",
    "\n",
    "    if rng is None:\n",
    "        rng = random.Random()\n",
    "\n",
    "    total_dims = 1\n",
    "    for dim in shape:\n",
    "        total_dims *= dim\n",
    "\n",
    "    values = []\n",
    "    for _ in range(total_dims):\n",
    "        values.append(rng.randint(0, vocab_size - 1))\n",
    "\n",
    "    return torch.tensor(data=values, dtype=torch.long).view(shape).contiguous()\n",
    "\n",
    "\n",
    "def get_llama_model(\n",
    "    input_dims=[(2, 1024)],\n",
    "    hidden_size=1024,  # 4096,\n",
    "    num_hidden_layers=1,\n",
    "    vocab_size=32000,\n",
    "    intermediate_size=11008,\n",
    "    max_position_embeddings=2048,\n",
    "    num_attention_heads=4,  # 32,\n",
    "    _attn_implementation=\"eager\",\n",
    "    with_mask: bool = True,\n",
    "):\n",
    "    import torch\n",
    "    from transformers import LlamaConfig\n",
    "    from transformers.models.llama.modeling_llama import LlamaModel\n",
    "\n",
    "    config = LlamaConfig(\n",
    "        num_hidden_layers=num_hidden_layers,\n",
    "        vocab_size=vocab_size,\n",
    "        hidden_size=hidden_size,\n",
    "        intermediate_size=intermediate_size,\n",
    "        max_position_embeddings=max_position_embeddings,\n",
    "        num_attention_heads=num_attention_heads,\n",
    "    )\n",
    "    if _attn_implementation:\n",
    "        config._attn_implementation = _attn_implementation\n",
    "\n",
    "    class LlamaModelWrapper(torch.nn.Module):\n",
    "        def __init__(self, config):\n",
    "            super().__init__()\n",
    "            self.model = LlamaModel(config)\n",
    "\n",
    "        def forward(self, input_ids, attention_mask):\n",
    "            model_output = self.model(input_ids, attention_mask=attention_mask)\n",
    "            return model_output.to_tuple()\n",
    "\n",
    "    def generate_example_inputs(batch: int, seq: int, vocab_size: int):\n",
    "        input_ids = ids_tensor([batch, seq], vocab_size)\n",
    "        input_mask = torch.tril(torch.ones(batch, seq, dtype=torch.float32))\n",
    "        assert input_mask.dtype == torch.float32\n",
    "        return input_ids, input_mask\n",
    "\n",
    "    example_args_collection = []\n",
    "    for b, s in input_dims:\n",
    "        example_args_collection.append(generate_example_inputs(b, s, vocab_size))\n",
    "\n",
    "    return LlamaModelWrapper(config), example_args_collection\n",
    "\n",
    "\n",
    "print(\"creation of the model.\")\n",
    "model, example_args_collection = get_llama_model()\n",
    "print(\"done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaModelWrapper(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 1024)\n",
      "    (layers): ModuleList(\n",
      "      (0): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=1024, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=1024, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=1024, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Module\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-1.1170, -0.7974, -1.9838,  ...,  0.6843,  1.6093, -0.8728],\n",
       "          [-1.1460, -0.9141, -2.0375,  ...,  0.6653,  1.5838, -0.8952],\n",
       "          [-1.1642, -1.0227, -1.9189,  ...,  0.7274,  1.6082, -0.8445],\n",
       "          ...,\n",
       "          [-1.2189, -0.9158, -1.9213,  ...,  0.7216,  1.5668, -0.8586],\n",
       "          [-1.1780, -0.9684, -1.8898,  ...,  0.7232,  1.4959, -0.9167],\n",
       "          [-1.1783, -0.8946, -1.9557,  ...,  0.8079,  1.5094, -0.8847]],\n",
       " \n",
       "         [[ 0.0058,  1.1115, -0.4164,  ...,  1.2131,  0.0819, -1.4715],\n",
       "          [-0.9829,  1.3897,  0.8368,  ..., -0.1238, -0.0239, -1.1874],\n",
       "          [-2.0979, -0.1517,  0.1348,  ..., -0.5682, -0.5758, -0.7007],\n",
       "          ...,\n",
       "          [-0.8932, -0.5153, -0.9346,  ...,  0.8343, -0.2991, -0.7240],\n",
       "          [-0.2193,  0.0176,  0.0269,  ...,  0.3472, -0.6515, -1.4265],\n",
       "          [-1.1566,  0.6324,  0.8453,  ..., -0.3906, -0.5370, -1.0049]]],\n",
       "        grad_fn=<MulBackward0>),\n",
       " ((tensor([[[[-3.5316e-01, -2.8751e-01,  2.6478e-01,  ...,  1.0172e+00,\n",
       "               9.6761e-01, -1.0456e-01],\n",
       "             [ 7.7318e-01,  7.3106e-01,  1.7520e+00,  ...,  1.3157e+00,\n",
       "              -2.3294e-01, -2.2196e-01],\n",
       "             [ 3.9455e-01,  1.9253e-02,  2.3313e-01,  ...,  1.5940e-01,\n",
       "              -8.2179e-01,  6.3996e-01],\n",
       "             ...,\n",
       "             [ 3.1977e-01, -1.1807e-01,  5.2624e-01,  ...,  1.8027e-01,\n",
       "               1.9409e-01, -1.0602e+00],\n",
       "             [ 8.6384e-02, -1.4486e+00,  1.7317e-01,  ..., -4.4551e-02,\n",
       "              -5.8946e-01,  7.6557e-01],\n",
       "             [ 1.5739e+00, -7.2942e-02, -6.7011e-01,  ..., -3.2602e-01,\n",
       "               2.3632e-01, -3.7781e-01]],\n",
       "   \n",
       "            [[-7.5511e-01, -6.9640e-02, -2.4332e-01,  ...,  7.8115e-01,\n",
       "              -1.3010e-01,  3.0747e-04],\n",
       "             [-4.1202e-01,  2.9124e-01, -7.2529e-01,  ...,  1.0000e-01,\n",
       "              -4.3358e-01,  1.1382e-01],\n",
       "             [-7.7963e-02, -3.7617e-01,  3.1977e-02,  ..., -3.1209e-01,\n",
       "               2.0622e-01, -7.9747e-02],\n",
       "             ...,\n",
       "             [-7.9156e-01,  3.5696e-01,  5.5794e-01,  ..., -2.9316e-01,\n",
       "              -1.1260e+00,  2.0917e-01],\n",
       "             [-5.4859e-01,  2.3112e-01,  4.8495e-01,  ...,  7.9071e-01,\n",
       "               2.8301e-02,  2.8595e-01],\n",
       "             [-4.4990e-01, -1.8011e-01, -7.4169e-02,  ...,  9.1768e-01,\n",
       "               7.2396e-01,  3.2855e-01]],\n",
       "   \n",
       "            [[ 4.3923e-01,  1.6130e-01,  6.4401e-01,  ..., -3.4096e-01,\n",
       "              -1.3136e-01,  9.4226e-01],\n",
       "             [-7.4927e-01, -1.0815e-01,  4.8161e-01,  ...,  2.6889e-01,\n",
       "              -8.6177e-01, -7.8413e-01],\n",
       "             [ 2.9678e-01, -1.4420e-01, -5.9051e-01,  ..., -4.9948e-01,\n",
       "               2.1471e-02,  1.7819e+00],\n",
       "             ...,\n",
       "             [-4.3322e-01, -1.2235e-01, -1.0945e+00,  ..., -2.0205e-01,\n",
       "               2.0096e-01, -6.7805e-01],\n",
       "             [ 2.3298e-01, -5.3079e-01, -2.8526e-01,  ..., -5.1613e-01,\n",
       "               1.4779e-02, -8.6390e-02],\n",
       "             [-4.7331e-01, -1.1903e-01,  4.8615e-02,  ..., -1.1487e+00,\n",
       "               2.1455e-01,  5.1834e-02]],\n",
       "   \n",
       "            [[ 1.3780e+00,  6.4414e-01, -1.8758e+00,  ...,  1.5249e-01,\n",
       "               2.5689e-01,  1.4628e+00],\n",
       "             [ 4.1881e-01, -9.1733e-02, -9.8733e-01,  ...,  2.6868e-02,\n",
       "               4.4279e-01, -3.8235e-01],\n",
       "             [ 3.6784e-01, -1.2619e-01, -5.1158e-01,  ..., -5.3879e-01,\n",
       "               3.1256e-01, -2.4376e-01],\n",
       "             ...,\n",
       "             [ 3.6018e-02,  1.6523e-01,  6.6273e-02,  ..., -1.2371e+00,\n",
       "               4.7622e-01,  2.9898e-01],\n",
       "             [ 2.4782e-01,  4.9735e-01,  1.4132e-01,  ..., -4.4838e-01,\n",
       "              -1.1823e-01, -1.0010e+00],\n",
       "             [-1.1307e-01,  5.1824e-01, -2.8621e-01,  ..., -7.9911e-01,\n",
       "               1.8540e-01, -1.4565e-01]]],\n",
       "   \n",
       "   \n",
       "           [[[ 4.4020e-01, -1.5832e+00,  8.2209e-01,  ..., -6.0377e-01,\n",
       "              -7.3325e-01, -6.8059e-01],\n",
       "             [ 1.5485e-01, -1.0984e+00, -1.8758e-01,  ...,  1.0464e+00,\n",
       "               1.4780e+00,  2.3050e-02],\n",
       "             [-1.0197e+00, -7.1025e-01,  1.2073e+00,  ...,  3.3438e-01,\n",
       "               1.0273e+00,  2.9528e-01],\n",
       "             ...,\n",
       "             [ 7.0037e-01,  1.1283e+00,  8.2546e-01,  ...,  3.5610e-01,\n",
       "               1.0934e+00, -3.8983e-02],\n",
       "             [ 4.0416e-02,  1.8876e+00,  9.5969e-02,  ...,  5.4414e-01,\n",
       "               5.5081e-01, -5.4332e-01],\n",
       "             [ 7.9903e-01,  2.8992e-01,  1.7971e-01,  ...,  7.2767e-03,\n",
       "              -4.2552e-01, -2.1908e-01]],\n",
       "   \n",
       "            [[-4.3887e-01,  2.3935e-01,  2.0997e-01,  ..., -4.7475e-01,\n",
       "              -1.3854e+00, -5.2482e-01],\n",
       "             [-6.7574e-01, -1.1254e+00,  1.9929e-01,  ..., -3.6341e-01,\n",
       "               1.3375e+00,  9.4538e-01],\n",
       "             [-1.3997e+00, -4.3473e-01, -2.3011e-01,  ...,  7.6538e-01,\n",
       "               4.8602e-01,  7.0487e-02],\n",
       "             ...,\n",
       "             [ 2.2768e-01,  6.2902e-01, -5.6186e-01,  ...,  2.5842e-02,\n",
       "              -5.8280e-01, -1.8129e-01],\n",
       "             [-2.9958e-01,  1.0749e-01,  1.2167e+00,  ...,  1.6439e-02,\n",
       "               1.3240e+00, -5.2000e-01],\n",
       "             [ 9.1642e-01,  7.1458e-01,  9.3353e-02,  ...,  3.9106e-01,\n",
       "              -1.1040e+00, -1.4405e-01]],\n",
       "   \n",
       "            [[-8.3804e-01,  1.2969e-01,  2.6329e-01,  ...,  6.8588e-01,\n",
       "               7.1946e-01,  2.4212e-01],\n",
       "             [ 6.6817e-01,  1.4864e-01,  2.7006e-01,  ...,  2.6392e-01,\n",
       "              -1.2895e-01,  3.4471e-01],\n",
       "             [ 9.6729e-03, -3.5499e-01, -1.4905e+00,  ...,  1.0371e+00,\n",
       "              -3.0903e-01,  3.9306e-01],\n",
       "             ...,\n",
       "             [ 1.4633e-01,  2.8192e-01,  5.4084e-01,  ...,  8.4976e-01,\n",
       "              -3.6081e-01, -9.1746e-01],\n",
       "             [ 7.7206e-02, -2.0116e-01,  5.3510e-01,  ..., -1.2972e-01,\n",
       "              -1.5358e+00, -3.4194e-01],\n",
       "             [-2.3183e-02, -1.9718e-01,  7.5260e-01,  ...,  4.8280e-01,\n",
       "              -1.0161e+00,  5.7380e-01]],\n",
       "   \n",
       "            [[-3.8136e-01,  6.6588e-01,  3.5003e-01,  ...,  8.4539e-02,\n",
       "              -5.4534e-01,  3.4974e-01],\n",
       "             [ 1.2873e-01, -4.4448e-02,  1.1140e+00,  ...,  1.4112e-01,\n",
       "               1.1244e-01, -1.1621e-01],\n",
       "             [-1.8855e-01,  6.6488e-01, -2.8346e-01,  ...,  9.2405e-02,\n",
       "              -3.9229e-01,  6.8475e-01],\n",
       "             ...,\n",
       "             [ 5.3947e-01,  2.3093e-01, -5.8483e-01,  ...,  8.4628e-02,\n",
       "              -6.4883e-01,  9.1407e-01],\n",
       "             [ 1.8131e+00,  1.4775e+00,  1.3825e+00,  ...,  7.1770e-02,\n",
       "               6.5652e-01, -3.4148e-01],\n",
       "             [-3.8478e-01,  4.0235e-01,  6.3744e-01,  ..., -1.0388e-01,\n",
       "              -2.9795e-01, -7.9301e-01]]]], grad_fn=<AddBackward0>),\n",
       "   tensor([[[[ 0.8143,  0.4651,  0.8287,  ...,  1.2225,  0.0048, -0.5485],\n",
       "             [ 0.1478, -0.2751, -0.3765,  ..., -0.2808,  0.5242, -0.5744],\n",
       "             [ 1.5593,  0.2561, -0.2828,  ..., -0.3193, -0.2892,  0.3511],\n",
       "             ...,\n",
       "             [ 0.1691, -0.3934, -1.0240,  ..., -1.5260, -0.7828,  0.4674],\n",
       "             [-0.9332,  0.6933,  0.9134,  ..., -0.2902, -0.2762,  0.2931],\n",
       "             [ 0.2897, -0.3924, -0.0328,  ...,  1.1862,  0.3280, -0.7029]],\n",
       "   \n",
       "            [[ 0.7096,  0.6004,  1.1923,  ..., -0.8298,  0.4507, -0.0881],\n",
       "             [ 0.0502, -0.5517, -0.2414,  ...,  0.4932,  0.6864, -0.4002],\n",
       "             [ 0.9480,  0.8523,  0.7194,  ..., -0.0727,  0.1866,  1.3464],\n",
       "             ...,\n",
       "             [-0.7335,  0.7594,  1.0372,  ..., -0.7147, -0.7901,  0.2385],\n",
       "             [-0.1925, -0.5970, -0.9114,  ..., -0.5501, -0.2581,  0.0097],\n",
       "             [-0.1639, -0.2898,  0.9469,  ...,  0.1044, -0.4026, -0.2129]],\n",
       "   \n",
       "            [[ 0.0997,  0.1349, -0.3606,  ..., -0.2062, -0.3327, -0.2660],\n",
       "             [ 0.0606, -0.1865,  1.0279,  ...,  0.2710, -0.5142, -0.4269],\n",
       "             [-1.3364,  0.3612, -0.3441,  ...,  1.0863, -1.1655, -1.5695],\n",
       "             ...,\n",
       "             [-0.4251,  0.0873, -0.8019,  ..., -0.5937, -0.3121,  1.2052],\n",
       "             [-0.3425,  0.0957,  0.5087,  ..., -0.6849,  0.1440, -0.0386],\n",
       "             [ 0.0788,  0.1732,  0.7895,  ..., -0.0857, -0.0954,  0.0378]],\n",
       "   \n",
       "            [[ 0.5637, -0.0933,  0.8772,  ...,  1.1306, -0.6008, -0.5730],\n",
       "             [ 0.0549,  1.0648, -0.2813,  ...,  0.5002,  0.7970, -0.7641],\n",
       "             [-0.3142,  1.1863,  0.5735,  ..., -0.3124,  0.5237,  0.8987],\n",
       "             ...,\n",
       "             [-0.3583, -0.9480, -0.1283,  ...,  0.7436,  0.0975,  1.1986],\n",
       "             [-0.2521,  0.6060,  0.0410,  ..., -0.2053, -0.1157, -0.1824],\n",
       "             [ 0.8881, -0.3809, -0.3057,  ..., -0.8265, -0.0433,  0.5633]]],\n",
       "   \n",
       "   \n",
       "           [[[-0.9176, -1.2247, -0.4625,  ...,  1.3659,  0.3266,  0.3222],\n",
       "             [-1.0802, -0.0082,  0.1546,  ..., -0.4047, -0.1166, -0.3582],\n",
       "             [ 0.4033,  0.4914,  1.1734,  ...,  0.4977, -0.9910, -0.2166],\n",
       "             ...,\n",
       "             [-0.4773,  0.0775, -0.1218,  ...,  0.2996,  0.4510, -0.4612],\n",
       "             [ 0.3576,  0.6056, -0.3581,  ..., -0.2187, -0.6140, -0.6608],\n",
       "             [ 0.2655, -0.3981,  0.8170,  ...,  0.5280, -0.8844,  0.7541]],\n",
       "   \n",
       "            [[-1.2006,  0.5386, -0.1198,  ..., -0.5554,  1.7841, -0.2983],\n",
       "             [-1.3000,  0.8906, -1.0256,  ..., -0.9805, -1.3691,  0.3944],\n",
       "             [ 0.0145, -0.6263, -0.7537,  ...,  0.1422,  0.3617,  0.0782],\n",
       "             ...,\n",
       "             [ 0.8600,  0.0091, -0.9357,  ...,  1.0383,  1.5403, -0.6475],\n",
       "             [-0.5531,  0.4177,  0.3305,  ...,  0.6726, -0.4085, -0.6145],\n",
       "             [ 0.5001,  0.0464,  0.0071,  ...,  0.6565,  0.6590, -0.1395]],\n",
       "   \n",
       "            [[-0.6881, -0.6278,  0.6012,  ...,  0.3505, -0.0734, -0.2735],\n",
       "             [-0.4625, -0.2566, -0.4162,  ...,  0.3256, -0.3635,  0.4041],\n",
       "             [ 0.2380,  0.0046, -0.5651,  ..., -0.0031,  0.4875, -0.1236],\n",
       "             ...,\n",
       "             [-0.1038,  0.1401,  0.7555,  ...,  0.4750,  0.3192,  0.8774],\n",
       "             [ 0.6575,  0.4482, -1.1116,  ..., -0.8120,  0.8972, -0.7654],\n",
       "             [-0.5853, -0.0757, -0.2246,  ..., -0.2865, -0.5225, -0.2367]],\n",
       "   \n",
       "            [[ 0.3377,  0.0183, -0.9681,  ..., -0.4807, -0.2547,  0.4951],\n",
       "             [-0.3002, -0.4928,  0.0367,  ...,  0.1695,  0.0389, -0.7301],\n",
       "             [-0.6336,  0.1779, -0.0234,  ...,  0.2888,  0.1705, -1.7139],\n",
       "             ...,\n",
       "             [-0.9192,  0.3187,  0.0216,  ..., -0.3082, -0.3016, -0.6210],\n",
       "             [-0.5091, -0.1146,  0.8452,  ...,  0.2147, -0.1397, -0.3283],\n",
       "             [ 0.3046,  1.6241, -0.0027,  ..., -0.0740, -0.2763, -0.1752]]]],\n",
       "          grad_fn=<TransposeBackward0>)),))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the model\n",
    "input_dim = (2, 1024)\n",
    "example_arg = example_args_collection[0]\n",
    "model(*example_args_collection[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About the dump model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The wrapping boils down to:\n",
    "```\n",
    "- wrapper\n",
    "    - model\n",
    "        - LlamaDecoderLayer\n",
    "```\n",
    "The model then is:  \n",
    "![Llama model](llama.jpg)\n",
    "\n",
    "Which corresponds on the netron model to:  \n",
    "![Netron labeled](llama_netron_labeled.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step by step\n",
    "from transformers.cache_utils import Cache, DynamicCache, StaticCache\n",
    "from transformers.modeling_outputs import BaseModelOutputWithPast\n",
    "\n",
    "# args\n",
    "input_ids, attention_mask = example_arg\n",
    "_model = model.model\n",
    "\n",
    "# Forward\n",
    "output_attentions = _model.config.output_attentions\n",
    "output_hidden_states = _model.config.output_hidden_states\n",
    "use_cache = _model.config.use_cache\n",
    "return_dict = _model.config.use_return_dict\n",
    "inputs_embeds = _model.embed_tokens(input_ids)\n",
    "\n",
    "past_seen_tokens = 0\n",
    "if not isinstance(None, StaticCache):\n",
    "    past_key_values = DynamicCache.from_legacy_cache(None)\n",
    "    past_seen_tokens = past_key_values.get_seq_length()\n",
    "\n",
    "if isinstance(past_key_values, StaticCache):\n",
    "    raise ValueError(\"cache_position is a required argument when using StaticCache.\")\n",
    "cache_position = torch.arange(\n",
    "    past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n",
    ")\n",
    "\n",
    "position_ids = cache_position.unsqueeze(0)\n",
    "causal_mask = _model._update_causal_mask(attention_mask, inputs_embeds, cache_position, past_seen_tokens)\n",
    "\n",
    "# embed positions\n",
    "hidden_states = inputs_embeds\n",
    "\n",
    "# decoder layers\n",
    "all_hidden_states = None\n",
    "all_self_attns = None\n",
    "next_decoder_cache = None\n",
    "\n",
    "decoder_layer = _model.layers[0]\n",
    "\n",
    "# forward from decoder layer\n",
    "attention_mask=causal_mask\n",
    "position_ids=position_ids\n",
    "past_key_value=past_key_values\n",
    "output_attentions=output_attentions\n",
    "use_cache=use_cache\n",
    "cache_position=cache_position\n",
    "\n",
    "residual = hidden_states\n",
    "hidden_states = decoder_layer.input_layernorm(hidden_states) # hidden_states is x\n",
    "\n",
    "# Self Attention\n",
    "hidden_states, self_attn_weights, present_key_value = decoder_layer.self_attn(\n",
    "    hidden_states=hidden_states,\n",
    "    attention_mask=attention_mask,\n",
    "    position_ids=position_ids,\n",
    "    past_key_value=past_key_value,\n",
    "    output_attentions=output_attentions,\n",
    "    use_cache=use_cache,\n",
    "    cache_position=cache_position\n",
    ")\n",
    "hidden_states = residual + hidden_states\n",
    "\n",
    "# Fully Connected\n",
    "residual = hidden_states\n",
    "hidden_states = decoder_layer.post_attention_layernorm(hidden_states)\n",
    "mlp_input = hidden_states # for mlp fuzed kernel\n",
    "hidden_states = decoder_layer.mlp(hidden_states)\n",
    "hidden_states = residual + hidden_states\n",
    "\n",
    "outputs = (hidden_states,)\n",
    "\n",
    "# if output_attentions:\n",
    "#     outputs += (self_attn_weights,)\n",
    "\n",
    "outputs += (present_key_value,)\n",
    "\n",
    "layer_outputs = outputs\n",
    "# end of forward from decoder layer\n",
    "\n",
    "hidden_states = layer_outputs[0]\n",
    "\n",
    "next_decoder_cache = layer_outputs[2 if output_attentions else 1]\n",
    "\n",
    "hidden_states = _model.norm(hidden_states)\n",
    "\n",
    "# add hidden states from the last decoder layer\n",
    "if output_hidden_states:\n",
    "    all_hidden_states += (hidden_states,)\n",
    "\n",
    "next_cache = None\n",
    "if use_cache:\n",
    "    next_cache = (\n",
    "        next_decoder_cache.to_legacy_cache() if isinstance(next_decoder_cache, Cache) else next_decoder_cache\n",
    "    )\n",
    "\n",
    "_return = BaseModelOutputWithPast(\n",
    "last_hidden_state=hidden_states,\n",
    "past_key_values=next_cache,\n",
    "hidden_states=all_hidden_states,\n",
    "attentions=all_self_attns,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mlp fuzed kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate the MLP\n",
    "mlp = decoder_layer.mlp\n",
    "torch.onnx.export(\n",
    "    mlp,\n",
    "    (mlp_input,),\n",
    "    \"mlp.onnx\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `mlp` module actually correspond to a gated multi-layer perceptron.  \n",
    "**Compare Netron, the gMLP paper, the default implementation and your implementation.**  \n",
    "The gated MLP returns: `self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))`, that is to say:\n",
    "$$(\\sigma(X \\times G^T) \\odot (X \\times U^T)) \\times D^T$$\n",
    "With the shapes:\n",
    "- $X: n,d$\n",
    "- $U: \\tilde{d},d$\n",
    "- $G: \\tilde{d},d$\n",
    "- $D: d,\\tilde{d}$  \n",
    "\n",
    "$\\sigma$ corresponds to the $SiLU$ activation function: $\\sigma(x)=x*sigmoid(x)$\n",
    "\n",
    "The notations from the original gMLP paper are:  \n",
    "$$Z=\\sigma(X U), \\quad \\tilde{Z}=s(Z), \\quad Y=\\tilde{Z} V$$\n",
    "\n",
    "![MLP netron labeled](mlp_netron_labeled.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "hidden_size = _model.config.hidden_size\n",
    "intermediate_size = _model.config.intermediate_size\n",
    "mlp_input_shape = mlp_input.shape\n",
    "G = mlp.gate_proj.weight\n",
    "U = mlp.up_proj.weight\n",
    "D = mlp.down_proj.weight\n",
    "\n",
    "# Forward\n",
    "X = mlp_input\n",
    "output = mlp(mlp_input)\n",
    "act_fn = torch.nn.SiLU()\n",
    "_ = (act_fn(X@G.T)*(X@U.T)) @ D.T\n",
    "assert torch.allclose(output, _)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export\n",
    "torch.save(mlp,\"mlp.pt\")\n",
    "torch.save(mlp_input,\"mlp_input.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
