{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is not available. Using CPU instead.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA (GPU support) is available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available! You can use GPU acceleration.\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Using CPU instead.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dump model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creation of the model.\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "import contextlib\n",
    "import io\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "\n",
    "def ids_tensor(shape, vocab_size, rng=None, name=None):\n",
    "    #  Creates a random int32 tensor of the shape within the vocab size\n",
    "    import torch\n",
    "\n",
    "    if rng is None:\n",
    "        rng = random.Random()\n",
    "\n",
    "    total_dims = 1\n",
    "    for dim in shape:\n",
    "        total_dims *= dim\n",
    "\n",
    "    values = []\n",
    "    for _ in range(total_dims):\n",
    "        values.append(rng.randint(0, vocab_size - 1))\n",
    "\n",
    "    return torch.tensor(data=values, dtype=torch.long).view(shape).contiguous()\n",
    "\n",
    "\n",
    "def get_llama_model(\n",
    "    input_dims=[(2, 1024)],\n",
    "    hidden_size=1024,  # 4096,\n",
    "    num_hidden_layers=1,\n",
    "    vocab_size=32000,\n",
    "    intermediate_size=11008,\n",
    "    max_position_embeddings=2048,\n",
    "    num_attention_heads=4,  # 32,\n",
    "    _attn_implementation=\"eager\",\n",
    "    with_mask: bool = True,\n",
    "):\n",
    "    import torch\n",
    "    from transformers import LlamaConfig\n",
    "    from transformers.models.llama.modeling_llama import LlamaModel\n",
    "\n",
    "    config = LlamaConfig(\n",
    "        num_hidden_layers=num_hidden_layers,\n",
    "        vocab_size=vocab_size,\n",
    "        hidden_size=hidden_size,\n",
    "        intermediate_size=intermediate_size,\n",
    "        max_position_embeddings=max_position_embeddings,\n",
    "        num_attention_heads=num_attention_heads,\n",
    "    )\n",
    "    if _attn_implementation:\n",
    "        config._attn_implementation = _attn_implementation\n",
    "\n",
    "    class LlamaModelWrapper(torch.nn.Module):\n",
    "        def __init__(self, config):\n",
    "            super().__init__()\n",
    "            self.model = LlamaModel(config)\n",
    "\n",
    "        def forward(self, input_ids, attention_mask):\n",
    "            model_output = self.model(input_ids, attention_mask=attention_mask)\n",
    "            return model_output.to_tuple()\n",
    "\n",
    "    def generate_example_inputs(batch: int, seq: int, vocab_size: int):\n",
    "        input_ids = ids_tensor([batch, seq], vocab_size)\n",
    "        input_mask = torch.tril(torch.ones(batch, seq, dtype=torch.float32))\n",
    "        assert input_mask.dtype == torch.float32\n",
    "        return input_ids, input_mask\n",
    "\n",
    "    example_args_collection = []\n",
    "    for b, s in input_dims:\n",
    "        example_args_collection.append(generate_example_inputs(b, s, vocab_size))\n",
    "\n",
    "    return LlamaModelWrapper(config), example_args_collection\n",
    "\n",
    "\n",
    "print(\"creation of the model.\")\n",
    "model, example_args_collection = get_llama_model()\n",
    "print(\"done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaModelWrapper(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 1024)\n",
      "    (layers): ModuleList(\n",
      "      (0): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=1024, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=1024, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=1024, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Module\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.5964,  0.0852, -2.6697,  ..., -0.4267, -0.2268, -0.0177],\n",
       "          [-0.7002,  0.1764, -2.6359,  ..., -0.5360, -0.2090, -0.0687],\n",
       "          [-0.7079,  0.1302, -2.5654,  ..., -0.4993, -0.2569,  0.0430],\n",
       "          ...,\n",
       "          [-0.7121,  0.1497, -2.8036,  ..., -0.4497, -0.2452, -0.1083],\n",
       "          [-0.6879,  0.0596, -2.5732,  ..., -0.5863, -0.3104,  0.0370],\n",
       "          [-0.6248,  0.1706, -2.6757,  ..., -0.5379, -0.3801,  0.0656]],\n",
       " \n",
       "         [[-0.9653, -0.7956,  0.1263,  ..., -0.3280, -0.6299, -0.4557],\n",
       "          [-0.5577, -0.5082, -0.5937,  ...,  0.5286,  0.1336, -0.7083],\n",
       "          [-0.5141, -0.7747, -0.8317,  ...,  0.9618,  0.5471, -0.4584],\n",
       "          ...,\n",
       "          [-0.6812, -0.6452, -0.3749,  ...,  0.4422,  1.0973,  0.4381],\n",
       "          [-0.8769, -0.9723, -0.5282,  ...,  0.0245,  0.7614,  0.1072],\n",
       "          [-0.7555, -1.0330, -0.5950,  ...,  0.2457, -0.3474, -0.4372]]],\n",
       "        grad_fn=<MulBackward0>),\n",
       " ((tensor([[[[-9.6755e-02, -8.4069e-01,  2.8660e-01,  ...,  9.5022e-01,\n",
       "              -9.5231e-01, -2.9445e-01],\n",
       "             [ 7.2726e-01, -1.6667e-01,  5.9501e-01,  ...,  3.1672e-02,\n",
       "              -3.1443e-01, -3.3322e-01],\n",
       "             [-1.1576e-01, -2.6894e-01, -4.6121e-01,  ...,  2.6690e-01,\n",
       "              -3.1895e-01, -1.7123e-01],\n",
       "             ...,\n",
       "             [ 9.7303e-01,  1.2711e-01,  3.6475e-01,  ...,  5.8543e-01,\n",
       "               6.6041e-02,  1.0733e-01],\n",
       "             [ 6.1180e-01, -5.8759e-02, -8.6505e-01,  ..., -1.4573e-01,\n",
       "               7.9853e-01, -1.2967e+00],\n",
       "             [ 8.8157e-01,  7.2033e-01,  1.0364e+00,  ..., -5.7839e-01,\n",
       "              -3.9378e-01, -8.7943e-01]],\n",
       "   \n",
       "            [[ 5.1280e-01, -6.4481e-01,  4.9513e-02,  ...,  4.5699e-01,\n",
       "               1.1088e+00,  1.4752e-01],\n",
       "             [ 4.5270e-01, -2.2327e-01, -1.1279e+00,  ...,  7.0033e-01,\n",
       "               1.5874e+00, -9.8336e-01],\n",
       "             [ 1.8422e-01, -1.5979e+00, -5.9070e-01,  ...,  4.3578e-01,\n",
       "               5.5158e-01,  9.5258e-03],\n",
       "             ...,\n",
       "             [ 4.8307e-01,  6.0178e-01,  9.9094e-01,  ...,  1.1524e+00,\n",
       "              -4.0675e-01,  2.9881e-01],\n",
       "             [-3.5058e-02,  1.5762e-01, -2.9803e-01,  ...,  2.9555e-02,\n",
       "               7.8381e-01, -9.0399e-01],\n",
       "             [-6.2135e-01,  1.7365e-01, -3.6585e-01,  ..., -1.5012e-01,\n",
       "              -1.0927e+00,  5.8053e-01]],\n",
       "   \n",
       "            [[ 8.2214e-01, -8.8414e-01, -1.7749e-01,  ...,  8.3473e-01,\n",
       "              -6.8405e-02, -5.4050e-01],\n",
       "             [ 4.0015e-01, -4.6570e-01,  4.7308e-01,  ..., -1.4312e+00,\n",
       "               7.3492e-02,  2.4456e+00],\n",
       "             [ 1.2872e+00,  7.5532e-02,  1.9728e-01,  ...,  7.6170e-01,\n",
       "              -3.2916e-01,  4.4264e-01],\n",
       "             ...,\n",
       "             [ 5.3277e-01, -8.7290e-01, -2.1964e-01,  ..., -4.9348e-01,\n",
       "               1.2803e+00,  1.2385e+00],\n",
       "             [ 1.6975e-01,  8.3378e-01, -5.4216e-01,  ..., -8.3485e-01,\n",
       "              -6.8430e-01,  7.6316e-01],\n",
       "             [-1.0760e+00,  5.7738e-02, -1.7859e-01,  ..., -7.6857e-01,\n",
       "              -6.5651e-01,  1.0461e-01]],\n",
       "   \n",
       "            [[-1.6814e-01,  1.3792e+00,  5.2668e-01,  ..., -3.2895e-01,\n",
       "               6.8859e-01, -3.2204e-02],\n",
       "             [-3.7750e-01, -8.6171e-01,  1.2902e+00,  ...,  4.8698e-01,\n",
       "              -3.3883e-01, -1.6893e-01],\n",
       "             [-1.2259e-01,  7.7425e-01,  8.7234e-01,  ..., -1.2155e-01,\n",
       "              -6.0889e-01, -2.6790e-01],\n",
       "             ...,\n",
       "             [ 3.9221e-01, -8.8889e-01,  5.2513e-01,  ...,  1.8776e-01,\n",
       "              -9.6759e-01, -1.3209e+00],\n",
       "             [ 2.2886e-03,  1.2092e-01,  6.7701e-01,  ...,  2.3779e-01,\n",
       "               3.6662e-01,  3.1273e-01],\n",
       "             [-1.1600e+00,  2.1684e-01, -1.0439e-01,  ..., -5.4439e-01,\n",
       "              -1.4834e-01, -2.4635e-01]]],\n",
       "   \n",
       "   \n",
       "           [[[-9.8253e-02,  1.1133e+00, -1.5815e+00,  ...,  1.4765e+00,\n",
       "               9.3347e-01,  1.0196e+00],\n",
       "             [-1.9334e-01,  6.5068e-01, -1.5833e-01,  ..., -1.8188e-01,\n",
       "              -4.1887e-01, -5.0393e-01],\n",
       "             [ 9.8026e-01,  1.2201e-03,  6.0874e-01,  ...,  6.2837e-01,\n",
       "              -6.3014e-01, -7.8420e-01],\n",
       "             ...,\n",
       "             [-3.6118e-01,  2.9741e-01,  1.7317e-01,  ...,  6.0854e-01,\n",
       "              -8.0346e-01, -1.4876e-01],\n",
       "             [-3.2663e-01,  4.2136e-01, -6.0020e-01,  ..., -6.6124e-01,\n",
       "               1.6093e+00,  7.5620e-01],\n",
       "             [-1.6833e-01, -1.3382e-01, -1.0304e+00,  ..., -1.9824e-02,\n",
       "              -5.9110e-01, -1.9526e-01]],\n",
       "   \n",
       "            [[ 8.6184e-01,  9.3341e-02,  1.5288e-01,  ..., -5.2024e-01,\n",
       "              -1.6939e+00,  2.9017e-01],\n",
       "             [ 1.9937e-01,  1.4608e-01,  2.8663e-01,  ..., -8.8186e-01,\n",
       "              -7.7272e-01,  6.5946e-01],\n",
       "             [-5.9999e-01,  6.7549e-01,  9.8655e-02,  ...,  9.7388e-01,\n",
       "               1.6941e-01,  1.1421e+00],\n",
       "             ...,\n",
       "             [-9.1625e-01,  2.9858e-03, -3.5826e-01,  ..., -3.9952e-01,\n",
       "               9.1722e-01, -1.6253e-01],\n",
       "             [ 5.0807e-01, -6.4587e-01,  7.9408e-02,  ...,  1.3919e-01,\n",
       "              -1.2273e+00, -3.1049e-01],\n",
       "             [ 5.4502e-01,  9.6200e-01, -7.5242e-01,  ...,  5.8652e-01,\n",
       "              -1.9112e-01, -1.7072e+00]],\n",
       "   \n",
       "            [[ 2.0522e-01, -2.9340e-01, -3.1970e-01,  ..., -6.9313e-01,\n",
       "               1.6814e-01, -4.8230e-02],\n",
       "             [ 6.2619e-02,  6.4810e-01, -7.2156e-01,  ..., -7.2472e-02,\n",
       "               1.0498e-01,  1.6497e-01],\n",
       "             [ 4.2762e-01,  7.6432e-02,  2.3064e-01,  ..., -4.8980e-01,\n",
       "              -3.0754e-01,  1.0432e+00],\n",
       "             ...,\n",
       "             [-3.5895e-01, -8.6079e-02,  9.0832e-02,  ...,  2.1802e-01,\n",
       "               7.2682e-01, -8.2242e-01],\n",
       "             [ 5.8327e-02,  1.4948e-01, -1.5263e-01,  ...,  5.1410e-01,\n",
       "               8.6977e-01,  1.5277e-01],\n",
       "             [-1.7832e-01, -1.4974e-01,  5.0191e-01,  ..., -1.7197e-01,\n",
       "               9.0301e-01,  3.2604e-01]],\n",
       "   \n",
       "            [[-2.5011e-01, -4.2783e-02, -1.0177e+00,  ..., -1.7931e-01,\n",
       "              -1.1860e-01, -8.1702e-01],\n",
       "             [ 1.7411e-01,  5.0496e-01, -1.0807e+00,  ...,  3.2251e-03,\n",
       "              -2.1934e-01, -8.2386e-01],\n",
       "             [-6.9769e-01,  1.8179e-02, -1.6522e-01,  ...,  2.4360e-02,\n",
       "               2.5550e-01, -2.1554e-01],\n",
       "             ...,\n",
       "             [ 6.9338e-01,  1.4524e-01,  7.2588e-01,  ..., -8.0165e-01,\n",
       "              -8.8542e-01, -3.8189e-01],\n",
       "             [-8.7414e-01,  3.2726e-01, -5.6026e-01,  ...,  3.0756e-01,\n",
       "               1.1457e-01, -9.0812e-01],\n",
       "             [-8.2210e-01,  7.3329e-01,  8.8430e-01,  ...,  1.0369e-01,\n",
       "               5.0661e-01,  1.8650e+00]]]], grad_fn=<AddBackward0>),\n",
       "   tensor([[[[ 0.4347,  0.5499, -0.7165,  ...,  0.1973, -1.1918, -0.1871],\n",
       "             [-0.1932, -1.0432, -0.7831,  ..., -0.4093, -0.2412,  0.0888],\n",
       "             [ 0.3843,  0.5469, -0.7212,  ..., -0.6049,  0.9942, -0.1484],\n",
       "             ...,\n",
       "             [ 0.3689, -0.9204, -0.2704,  ...,  0.2318,  1.0910,  0.3000],\n",
       "             [-1.0774,  0.5107, -1.1611,  ..., -0.3481, -1.0367, -0.8886],\n",
       "             [-0.3121, -0.5409,  0.1327,  ...,  0.1753, -0.2683,  0.4477]],\n",
       "   \n",
       "            [[ 0.3433, -0.3050, -0.2826,  ..., -0.5906, -0.2759,  0.2043],\n",
       "             [-1.1859, -1.5484,  1.0042,  ..., -0.6288, -0.2240,  1.4297],\n",
       "             [-0.5111, -0.4346,  0.2501,  ...,  0.6544, -0.8354, -1.4113],\n",
       "             ...,\n",
       "             [-1.2125, -0.8381,  0.8420,  ...,  0.6342,  0.6555,  0.5892],\n",
       "             [ 0.4809,  0.7915, -0.7571,  ..., -0.9493,  0.0475, -0.9623],\n",
       "             [ 0.2492,  0.0908,  0.4060,  ...,  0.9862,  1.0906, -0.4301]],\n",
       "   \n",
       "            [[-0.8130, -0.4635, -1.0116,  ..., -0.5434,  1.5370, -0.0370],\n",
       "             [ 0.9999, -0.5594,  0.9726,  ..., -0.0466,  0.1975,  1.0867],\n",
       "             [-0.8163, -0.4317, -0.1933,  ...,  0.2622, -0.1968, -0.1491],\n",
       "             ...,\n",
       "             [-0.3343, -0.2908,  1.1866,  ..., -1.7319, -1.4955, -0.3800],\n",
       "             [ 0.1689, -0.4399, -0.0196,  ..., -0.6086,  0.1513,  0.1821],\n",
       "             [-0.1770,  1.2828, -0.1748,  ..., -0.1112,  1.0058,  0.1203]],\n",
       "   \n",
       "            [[ 1.1664,  0.1275,  0.8720,  ..., -0.4959, -0.1323, -0.4627],\n",
       "             [ 0.6112,  0.5581,  0.0866,  ..., -0.1408,  0.1140,  1.1540],\n",
       "             [-0.0886,  0.9075, -0.3450,  ...,  0.0660, -0.2202, -0.0565],\n",
       "             ...,\n",
       "             [ 0.8768,  0.1155,  0.3904,  ..., -0.5306, -0.5068,  0.3273],\n",
       "             [ 0.0663, -0.2963,  0.7550,  ...,  0.4563, -0.3408, -0.1142],\n",
       "             [-0.0659, -0.7942,  0.4053,  ..., -0.6769,  0.8420,  1.7015]]],\n",
       "   \n",
       "   \n",
       "           [[[-0.3204,  0.6735, -0.2905,  ..., -0.0755,  1.2001, -0.5579],\n",
       "             [ 0.2467,  0.2023,  1.9764,  ..., -0.1668,  0.7967,  0.3355],\n",
       "             [ 1.2828, -0.7273,  0.6911,  ..., -0.9187, -0.3355, -0.1468],\n",
       "             ...,\n",
       "             [ 0.7985, -0.7994,  0.3599,  ...,  0.1279,  0.3033, -0.6046],\n",
       "             [ 0.3655,  0.6527,  0.8456,  ...,  0.1053,  0.5160,  0.0794],\n",
       "             [-1.6799, -0.2033,  1.2133,  ..., -1.3131,  0.1618, -0.9585]],\n",
       "   \n",
       "            [[-0.8316, -0.9927, -0.3902,  ..., -0.4685,  0.5257, -0.9314],\n",
       "             [-0.9115,  1.0060, -0.1695,  ...,  0.0790, -2.0420, -0.2309],\n",
       "             [ 0.0412, -0.4669,  0.2278,  ...,  0.2157, -0.9110,  0.0941],\n",
       "             ...,\n",
       "             [-0.1532, -0.1349, -0.4506,  ...,  0.7524, -0.3344,  1.5765],\n",
       "             [-0.3619, -0.2430, -0.4871,  ..., -0.3314,  0.3962, -0.0169],\n",
       "             [-0.1271, -0.2566,  0.7063,  ...,  1.0077,  1.0819,  1.0543]],\n",
       "   \n",
       "            [[ 0.5433,  0.2735,  0.2561,  ...,  0.1058, -0.1088,  0.0039],\n",
       "             [ 0.1631, -0.4312, -0.6117,  ..., -0.9308, -0.5992,  0.3842],\n",
       "             [-0.3968,  0.1617, -1.0978,  ..., -0.0075,  0.2649,  0.1040],\n",
       "             ...,\n",
       "             [ 0.0041, -0.5773,  0.5950,  ..., -0.5761,  0.3875,  0.0790],\n",
       "             [ 0.5733, -0.9257, -0.4162,  ...,  0.4705, -0.3747,  0.4766],\n",
       "             [ 1.7457,  0.1345,  0.4180,  ...,  1.1223, -1.0888,  0.9391]],\n",
       "   \n",
       "            [[ 0.1338,  0.2859,  0.2789,  ..., -0.6734, -0.1667,  0.5772],\n",
       "             [ 1.2612, -0.7638,  0.2253,  ...,  0.4656, -0.0221,  0.2505],\n",
       "             [ 0.0712, -0.9796,  0.0697,  ...,  0.0962, -0.6415,  0.2450],\n",
       "             ...,\n",
       "             [ 0.3416, -0.0522, -0.5087,  ..., -0.9492, -0.0911, -0.1403],\n",
       "             [-1.1889, -0.9902, -0.5214,  ..., -0.6431,  0.7074, -0.5014],\n",
       "             [ 0.2800, -0.4304, -0.0413,  ...,  0.4153,  0.9416, -0.5028]]]],\n",
       "          grad_fn=<TransposeBackward0>)),))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the model\n",
    "input_dim = (2, 1024)\n",
    "example_arg = example_args_collection[0]\n",
    "model(*example_args_collection[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About the dump model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The wrapping boils down to:\n",
    "```\n",
    "- wrapper\n",
    "    - model\n",
    "        - LlamaDecoderLayer\n",
    "```\n",
    "The model then is:  \n",
    "![Llama model](llama.jpg)\n",
    "\n",
    "Which corresponds on the netron model to:  \n",
    "![Netron labeled](llama_netron_labeled.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "LlamaModel._update_causal_mask() missing 1 required positional argument: 'output_attentions'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 28\u001b[0m\n\u001b[0;32m     23\u001b[0m cache_position \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(\n\u001b[0;32m     24\u001b[0m     past_seen_tokens, past_seen_tokens \u001b[38;5;241m+\u001b[39m inputs_embeds\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], device\u001b[38;5;241m=\u001b[39minputs_embeds\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m     25\u001b[0m )\n\u001b[0;32m     27\u001b[0m position_ids \u001b[38;5;241m=\u001b[39m cache_position\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 28\u001b[0m causal_mask \u001b[38;5;241m=\u001b[39m \u001b[43m_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_causal_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_seen_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# embed positions\u001b[39;00m\n\u001b[0;32m     31\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m inputs_embeds\n",
      "\u001b[1;31mTypeError\u001b[0m: LlamaModel._update_causal_mask() missing 1 required positional argument: 'output_attentions'"
     ]
    }
   ],
   "source": [
    "# Step by step\n",
    "from transformers.cache_utils import Cache, DynamicCache, StaticCache\n",
    "from transformers.modeling_outputs import BaseModelOutputWithPast\n",
    "\n",
    "# args\n",
    "input_ids, attention_mask = example_arg\n",
    "_model = model.model\n",
    "\n",
    "# Forward\n",
    "output_attentions = _model.config.output_attentions\n",
    "output_hidden_states = _model.config.output_hidden_states\n",
    "use_cache = _model.config.use_cache\n",
    "return_dict = _model.config.use_return_dict\n",
    "inputs_embeds = _model.embed_tokens(input_ids)\n",
    "\n",
    "past_seen_tokens = 0\n",
    "if not isinstance(None, StaticCache):\n",
    "    past_key_values = DynamicCache.from_legacy_cache(None)\n",
    "    past_seen_tokens = past_key_values.get_seq_length()\n",
    "\n",
    "if isinstance(past_key_values, StaticCache):\n",
    "    raise ValueError(\"cache_position is a required argument when using StaticCache.\")\n",
    "cache_position = torch.arange(\n",
    "    past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n",
    ")\n",
    "\n",
    "position_ids = cache_position.unsqueeze(0)\n",
    "causal_mask = _model._update_causal_mask(attention_mask, inputs_embeds, cache_position, past_seen_tokens)\n",
    "\n",
    "# embed positions\n",
    "hidden_states = inputs_embeds\n",
    "\n",
    "# decoder layers\n",
    "all_hidden_states = None\n",
    "all_self_attns = None\n",
    "next_decoder_cache = None\n",
    "\n",
    "decoder_layer = _model.layers[0]\n",
    "\n",
    "# forward from decoder layer\n",
    "attention_mask=causal_mask\n",
    "position_ids=position_ids\n",
    "past_key_value=past_key_values\n",
    "output_attentions=output_attentions\n",
    "use_cache=use_cache\n",
    "cache_position=cache_position\n",
    "\n",
    "residual = hidden_states\n",
    "hidden_states = decoder_layer.input_layernorm(hidden_states) # hidden_states is x\n",
    "\n",
    "# Self Attention\n",
    "hidden_states, self_attn_weights, present_key_value = decoder_layer.self_attn(\n",
    "    hidden_states=hidden_states,\n",
    "    attention_mask=attention_mask,\n",
    "    position_ids=position_ids,\n",
    "    past_key_value=past_key_value,\n",
    "    output_attentions=output_attentions,\n",
    "    use_cache=use_cache,\n",
    "    cache_position=cache_position\n",
    ")\n",
    "hidden_states = residual + hidden_states\n",
    "\n",
    "# Fully Connected\n",
    "residual = hidden_states\n",
    "hidden_states = decoder_layer.post_attention_layernorm(hidden_states)\n",
    "mlp_input = hidden_states # for mlp fuzed kernel\n",
    "hidden_states = decoder_layer.mlp(hidden_states)\n",
    "hidden_states = residual + hidden_states\n",
    "\n",
    "outputs = (hidden_states,)\n",
    "\n",
    "# if output_attentions:\n",
    "#     outputs += (self_attn_weights,)\n",
    "\n",
    "outputs += (present_key_value,)\n",
    "\n",
    "layer_outputs = outputs\n",
    "# end of forward from decoder layer\n",
    "\n",
    "hidden_states = layer_outputs[0]\n",
    "\n",
    "next_decoder_cache = layer_outputs[2 if output_attentions else 1]\n",
    "\n",
    "hidden_states = _model.norm(hidden_states)\n",
    "\n",
    "# add hidden states from the last decoder layer\n",
    "if output_hidden_states:\n",
    "    all_hidden_states += (hidden_states,)\n",
    "\n",
    "next_cache = None\n",
    "if use_cache:\n",
    "    next_cache = (\n",
    "        next_decoder_cache.to_legacy_cache() if isinstance(next_decoder_cache, Cache) else next_decoder_cache\n",
    "    )\n",
    "\n",
    "_return = BaseModelOutputWithPast(\n",
    "last_hidden_state=hidden_states,\n",
    "past_key_values=next_cache,\n",
    "hidden_states=all_hidden_states,\n",
    "attentions=all_self_attns,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mlp fuzed kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'decoder_layer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Isolate the MLP\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m mlp \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[38;5;241m.\u001b[39mmlp\n\u001b[0;32m      3\u001b[0m torch\u001b[38;5;241m.\u001b[39monnx\u001b[38;5;241m.\u001b[39mexport(\n\u001b[0;32m      4\u001b[0m     mlp,\n\u001b[0;32m      5\u001b[0m     (mlp_input,),\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmlp.onnx\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      7\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'decoder_layer' is not defined"
     ]
    }
   ],
   "source": [
    "# Isolate the MLP\n",
    "mlp = decoder_layer.mlp\n",
    "torch.onnx.export(\n",
    "    mlp,\n",
    "    (mlp_input,),\n",
    "    \"mlp.onnx\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `mlp` module actually correspond to a gated multi-layer perceptron.  \n",
    "**Compare Netron, the gMLP paper, the default implementation and your implementation.**  \n",
    "The gated MLP returns: `self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))`, that is to say:\n",
    "$$(\\sigma(X \\times G^T) \\odot (X \\times U^T)) \\times D^T$$\n",
    "With the shapes:\n",
    "- $X: n,d$\n",
    "- $U: \\tilde{d},d$\n",
    "- $G: \\tilde{d},d$\n",
    "- $D: d,\\tilde{d}$  \n",
    "\n",
    "$\\sigma$ corresponds to the $SiLU$ activation function: $\\sigma(x)=x*sigmoid(x)$\n",
    "\n",
    "The notations from the original gMLP paper are:  \n",
    "$$Z=\\sigma(X U), \\quad \\tilde{Z}=s(Z), \\quad Y=\\tilde{Z} V$$\n",
    "\n",
    "![MLP netron labeled](mlp_netron_labeled.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "hidden_size = _model.config.hidden_size\n",
    "intermediate_size = _model.config.intermediate_size\n",
    "mlp_input_shape = mlp_input.shape\n",
    "G = mlp.gate_proj.weight\n",
    "U = mlp.up_proj.weight\n",
    "D = mlp.down_proj.weight\n",
    "\n",
    "# Forward\n",
    "X = mlp_input\n",
    "output = mlp(mlp_input)\n",
    "act_fn = torch.nn.SiLU()\n",
    "_ = (act_fn(X@G.T)*(X@U.T)) @ D.T\n",
    "assert torch.allclose(output, _)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export\n",
    "torch.save(mlp,\"mlp.pt\")\n",
    "torch.save(mlp_input,\"mlp_input.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
