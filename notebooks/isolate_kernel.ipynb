{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available! You can use GPU acceleration.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA (GPU support) is available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available! You can use GPU acceleration.\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Using CPU instead.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dump model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creation of the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Paul\\.conda\\envs\\GPU\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n"
     ]
    }
   ],
   "source": [
    "import contextlib\n",
    "import io\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "\n",
    "def ids_tensor(shape, vocab_size, rng=None, name=None):\n",
    "    #  Creates a random int32 tensor of the shape within the vocab size\n",
    "    import torch\n",
    "\n",
    "    if rng is None:\n",
    "        rng = random.Random()\n",
    "\n",
    "    total_dims = 1\n",
    "    for dim in shape:\n",
    "        total_dims *= dim\n",
    "\n",
    "    values = []\n",
    "    for _ in range(total_dims):\n",
    "        values.append(rng.randint(0, vocab_size - 1))\n",
    "\n",
    "    return torch.tensor(data=values, dtype=torch.long).view(shape).contiguous()\n",
    "\n",
    "\n",
    "def get_llama_model(\n",
    "    input_dims=[(2, 1024)],\n",
    "    hidden_size=1024,  # 4096,\n",
    "    num_hidden_layers=1,\n",
    "    vocab_size=32000,\n",
    "    intermediate_size=11008,\n",
    "    max_position_embeddings=2048,\n",
    "    num_attention_heads=4,  # 32,\n",
    "    _attn_implementation=\"eager\",\n",
    "    with_mask: bool = True,\n",
    "):\n",
    "    import torch\n",
    "    from transformers import LlamaConfig\n",
    "    from transformers.models.llama.modeling_llama import LlamaModel\n",
    "\n",
    "    config = LlamaConfig(\n",
    "        num_hidden_layers=num_hidden_layers,\n",
    "        vocab_size=vocab_size,\n",
    "        hidden_size=hidden_size,\n",
    "        intermediate_size=intermediate_size,\n",
    "        max_position_embeddings=max_position_embeddings,\n",
    "        num_attention_heads=num_attention_heads,\n",
    "    )\n",
    "    if _attn_implementation:\n",
    "        config._attn_implementation = _attn_implementation\n",
    "\n",
    "    class LlamaModelWrapper(torch.nn.Module):\n",
    "        def __init__(self, config):\n",
    "            super().__init__()\n",
    "            self.model = LlamaModel(config)\n",
    "\n",
    "        def forward(self, input_ids, attention_mask):\n",
    "            model_output = self.model(input_ids, attention_mask=attention_mask)\n",
    "            return model_output.to_tuple()\n",
    "\n",
    "    def generate_example_inputs(batch: int, seq: int, vocab_size: int):\n",
    "        input_ids = ids_tensor([batch, seq], vocab_size)\n",
    "        input_mask = torch.tril(torch.ones(batch, seq, dtype=torch.float32))\n",
    "        assert input_mask.dtype == torch.float32\n",
    "        return input_ids, input_mask\n",
    "\n",
    "    example_args_collection = []\n",
    "    for b, s in input_dims:\n",
    "        example_args_collection.append(generate_example_inputs(b, s, vocab_size))\n",
    "\n",
    "    return LlamaModelWrapper(config), example_args_collection\n",
    "\n",
    "\n",
    "print(\"creation of the model.\")\n",
    "model, example_args_collection = get_llama_model()\n",
    "print(\"done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaModelWrapper(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 1024)\n",
      "    (layers): ModuleList(\n",
      "      (0): LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=1024, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=1024, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=1024, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Module\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.6538,  1.4923, -1.0207,  ..., -0.9500, -0.6047,  0.2421],\n",
       "          [ 0.5902,  1.5998, -1.2004,  ..., -1.0070, -0.4677,  0.1595],\n",
       "          [ 0.5829,  1.4299, -1.1225,  ..., -1.1021, -0.3678,  0.2983],\n",
       "          ...,\n",
       "          [ 0.5308,  1.5105, -1.1224,  ..., -1.0443, -0.4936,  0.1287],\n",
       "          [ 0.4995,  1.5303, -1.1162,  ..., -1.1316, -0.5139,  0.1244],\n",
       "          [ 0.5986,  1.3492, -1.1980,  ..., -1.0503, -0.5308,  0.0821]],\n",
       " \n",
       "         [[ 1.0960,  0.2340,  0.7139,  ..., -0.1365, -0.6959,  1.3350],\n",
       "          [ 1.0510, -0.8339, -0.3548,  ...,  0.7212, -0.2489,  1.1318],\n",
       "          [ 0.8738,  0.0459,  0.1464,  ...,  0.4444, -0.5898,  1.5109],\n",
       "          ...,\n",
       "          [ 0.8249, -0.5053, -0.2931,  ...,  0.6251,  0.5112,  1.1570],\n",
       "          [ 0.7034, -0.7555, -0.4960,  ...,  0.9936,  0.5579,  0.5991],\n",
       "          [ 0.4533, -0.7071, -0.2579,  ...,  0.1992,  0.0167,  0.8224]]],\n",
       "        grad_fn=<MulBackward0>),\n",
       " ((tensor([[[[-0.1522,  0.6797,  0.9572,  ...,  0.3198, -0.9724,  0.0270],\n",
       "             [-0.8982,  1.1584, -0.5558,  ..., -0.7961, -0.7731,  0.9202],\n",
       "             [-0.9498, -0.1108,  0.9265,  ...,  0.5130, -0.3787,  0.9971],\n",
       "             ...,\n",
       "             [ 0.5406,  0.5220, -0.7081,  ..., -0.6143, -1.4030, -0.7854],\n",
       "             [-0.0938, -0.3428,  0.7025,  ...,  1.5998,  0.2646,  0.3482],\n",
       "             [ 0.1520, -0.5849, -0.4893,  ..., -1.1306, -0.3391,  0.8866]],\n",
       "   \n",
       "            [[-0.3647,  0.2871,  1.1669,  ..., -0.2317,  0.2916,  0.6731],\n",
       "             [-0.9253,  0.0783,  0.5985,  ..., -0.3110,  1.5224, -1.3543],\n",
       "             [ 0.5677, -1.0226, -0.2881,  ...,  0.8428,  1.1561,  0.2600],\n",
       "             ...,\n",
       "             [ 1.1509, -0.6425, -1.5416,  ..., -1.1121, -1.0140,  0.7616],\n",
       "             [ 0.7822, -0.8104,  0.3692,  ...,  0.9830, -0.5868,  0.4083],\n",
       "             [ 1.2662,  0.2304, -1.0390,  ..., -0.4141,  0.0071, -1.2477]],\n",
       "   \n",
       "            [[-0.2963, -0.2265, -1.0068,  ...,  0.2515, -0.0691, -0.1537],\n",
       "             [-0.6790, -0.4577, -0.2099,  ...,  0.5945, -0.1185, -0.9355],\n",
       "             [-0.2654, -0.1967, -0.2409,  ..., -0.8647, -1.2839, -0.2826],\n",
       "             ...,\n",
       "             [-0.2703,  0.7834,  0.2656,  ...,  0.6617, -0.9708,  0.1143],\n",
       "             [-0.1440, -0.2399, -1.0749,  ...,  0.1716,  1.1053,  0.3916],\n",
       "             [-0.1210, -0.5000, -0.6965,  ...,  0.9674, -0.6996,  0.1707]],\n",
       "   \n",
       "            [[ 0.6581, -0.2447,  0.9475,  ..., -0.7208, -0.2393,  0.0211],\n",
       "             [ 0.9801,  0.1324,  0.1546,  ...,  0.4445,  0.2971, -0.7396],\n",
       "             [ 0.4611,  0.5940, -1.0933,  ...,  0.4149,  0.7231,  0.3538],\n",
       "             ...,\n",
       "             [ 0.7293,  1.4991,  0.1010,  ...,  0.7585,  0.3529, -1.1427],\n",
       "             [-0.6912,  0.3884, -0.9433,  ...,  0.3964, -0.0106,  0.9309],\n",
       "             [ 0.1778, -0.0704, -0.2636,  ..., -1.1540,  0.4283, -0.7026]]],\n",
       "   \n",
       "   \n",
       "           [[[ 0.2421,  0.3216,  0.4949,  ...,  1.1544,  0.1000, -0.0198],\n",
       "             [ 0.3425, -0.8450, -0.5063,  ..., -0.6343,  0.9557,  0.4400],\n",
       "             [ 0.6073, -1.1293, -0.9229,  ...,  0.1898, -0.1750,  1.9103],\n",
       "             ...,\n",
       "             [-0.1481,  0.1530,  0.0288,  ...,  0.3737,  0.3279,  0.0089],\n",
       "             [ 0.5856, -0.4208, -0.1189,  ..., -0.0568, -0.3202,  0.5640],\n",
       "             [ 0.9089, -0.4175, -1.9767,  ..., -0.8978, -1.1576,  0.4988]],\n",
       "   \n",
       "            [[-0.3662, -0.5388,  0.7574,  ..., -1.2590, -0.8518, -0.3290],\n",
       "             [ 1.9837, -1.4891, -0.1983,  ..., -0.2221, -0.8950, -0.0606],\n",
       "             [-0.3515, -0.4406, -0.5807,  ...,  0.1041,  0.0585, -0.0579],\n",
       "             ...,\n",
       "             [ 1.1603,  0.2036,  0.2749,  ...,  0.5518,  1.4591, -0.0902],\n",
       "             [ 1.3433, -0.8270,  0.5222,  ..., -0.1179, -0.3325, -0.2231],\n",
       "             [-0.6560, -0.1284, -1.3141,  ...,  0.3641,  0.1679, -0.3251]],\n",
       "   \n",
       "            [[ 0.1586, -0.3887, -0.0138,  ...,  0.5053,  0.4456,  0.7710],\n",
       "             [-0.4750, -0.3569, -0.1105,  ..., -0.4070,  1.0755,  0.2174],\n",
       "             [-0.1877, -0.7963, -0.4515,  ...,  0.2059,  0.0905,  0.1908],\n",
       "             ...,\n",
       "             [-0.4496,  0.0822, -0.9236,  ...,  0.1671, -0.0395,  0.5350],\n",
       "             [-0.3864,  0.3199, -1.1792,  ...,  0.6564,  0.1608, -0.7033],\n",
       "             [-0.9509, -0.0415,  0.1317,  ..., -0.2356,  0.4604,  0.2644]],\n",
       "   \n",
       "            [[ 0.3479,  0.5922,  0.2229,  ..., -0.9844,  0.4652,  0.3123],\n",
       "             [-0.0792, -0.7898,  0.0964,  ...,  0.6061, -0.4153, -0.4038],\n",
       "             [-0.5382,  0.7604,  0.1735,  ...,  0.5424,  0.1207,  0.5263],\n",
       "             ...,\n",
       "             [-0.9291, -0.0657,  0.8926,  ...,  0.3518,  0.8136, -0.5614],\n",
       "             [ 0.6764, -0.6454,  0.3871,  ...,  0.2096, -0.0786,  0.6413],\n",
       "             [ 0.4656, -0.7319,  0.2907,  ...,  1.1485, -0.2576,  0.4214]]]],\n",
       "          grad_fn=<AddBackward0>),\n",
       "   tensor([[[[-4.4402e-01, -1.0175e+00,  9.1148e-02,  ...,  2.5197e-01,\n",
       "              -4.9319e-01, -2.4149e-01],\n",
       "             [-4.2163e-01, -4.5850e-01, -3.9972e-01,  ..., -1.2752e+00,\n",
       "              -1.5774e+00,  8.4547e-01],\n",
       "             [-2.1203e-01,  1.0948e+00, -5.8179e-01,  ...,  2.8085e-01,\n",
       "              -2.7962e-01, -1.1079e+00],\n",
       "             ...,\n",
       "             [-1.9657e-01,  6.5758e-01,  3.1433e-01,  ...,  1.0212e+00,\n",
       "               5.7773e-01,  3.3347e-01],\n",
       "             [-3.0514e-01, -3.6459e-01,  5.7955e-01,  ...,  1.2983e-03,\n",
       "              -2.0163e-01, -5.1230e-01],\n",
       "             [-1.6155e+00,  5.7803e-01, -4.4792e-01,  ..., -1.3505e-01,\n",
       "              -9.9232e-01,  1.0963e+00]],\n",
       "   \n",
       "            [[-1.2912e+00, -2.8767e-01,  2.8179e-01,  ..., -3.6174e-01,\n",
       "              -1.2295e+00, -2.4752e-01],\n",
       "             [ 7.3137e-01,  2.2924e-01,  1.1638e+00,  ..., -1.1387e-01,\n",
       "              -9.3157e-01,  2.3447e-01],\n",
       "             [-2.7014e-01, -4.9525e-01,  2.6432e-01,  ..., -6.8241e-02,\n",
       "              -7.5402e-02, -5.3262e-01],\n",
       "             ...,\n",
       "             [-1.7491e-01, -3.5490e-01, -2.0291e+00,  ..., -1.0394e+00,\n",
       "              -4.7538e-01, -5.5909e-01],\n",
       "             [ 4.1028e-01,  5.6917e-01, -2.4922e-01,  ...,  1.0943e+00,\n",
       "              -2.2984e-01, -5.7520e-01],\n",
       "             [ 5.9534e-01,  8.4455e-01,  2.4417e-02,  ...,  6.0319e-02,\n",
       "               1.2648e+00, -3.5111e-02]],\n",
       "   \n",
       "            [[ 3.3218e-01, -1.3334e+00, -1.8349e+00,  ...,  7.8185e-01,\n",
       "               5.9109e-01,  2.5353e-01],\n",
       "             [-3.8821e-01,  9.0365e-02,  4.7921e-01,  ..., -5.3487e-01,\n",
       "              -7.9662e-01,  2.2086e-01],\n",
       "             [-3.1945e-01,  4.6726e-01, -2.0306e+00,  ..., -7.9326e-02,\n",
       "               4.0620e-02,  3.6082e-02],\n",
       "             ...,\n",
       "             [ 1.0131e+00,  1.1376e+00,  2.6462e-02,  ...,  1.0298e+00,\n",
       "              -5.4834e-01, -6.8710e-01],\n",
       "             [-1.0026e-02,  4.9248e-01,  1.5146e-01,  ...,  4.9030e-01,\n",
       "              -1.1996e-01,  9.4328e-01],\n",
       "             [ 1.0112e+00,  1.9234e-01, -6.5846e-01,  ...,  1.7346e-01,\n",
       "               4.6942e-01, -4.1120e-01]],\n",
       "   \n",
       "            [[ 4.1247e-01,  1.5242e-01,  1.6630e-01,  ...,  1.0232e+00,\n",
       "              -1.9950e-02,  1.6603e-01],\n",
       "             [-7.0589e-01,  1.4931e-01, -5.9726e-01,  ..., -1.1928e+00,\n",
       "               4.5255e-01,  6.4027e-01],\n",
       "             [-1.1046e-01,  8.1646e-01, -1.0823e+00,  ..., -2.2440e-01,\n",
       "              -3.7183e-01, -4.1764e-02],\n",
       "             ...,\n",
       "             [ 5.5640e-01, -2.0723e-01,  6.7524e-03,  ..., -8.8652e-01,\n",
       "              -4.7740e-01,  1.5795e-01],\n",
       "             [-2.9529e-01,  9.7715e-01,  1.4729e-01,  ..., -7.3418e-02,\n",
       "              -4.5378e-01, -2.2538e-01],\n",
       "             [-4.4399e-01, -9.7419e-01,  4.0339e-01,  ...,  5.0136e-01,\n",
       "              -4.7006e-01, -1.0616e+00]]],\n",
       "   \n",
       "   \n",
       "           [[[-8.0140e-01, -1.3329e-01, -2.0922e-01,  ..., -5.4191e-01,\n",
       "              -3.4175e-01, -4.3499e-01],\n",
       "             [ 1.6413e-01, -1.3252e+00,  1.0164e+00,  ..., -6.5558e-02,\n",
       "               5.6167e-01, -1.6687e+00],\n",
       "             [-4.1968e-01,  1.1284e-01,  1.9298e-02,  ...,  8.5203e-01,\n",
       "               2.8900e-02,  1.0845e-01],\n",
       "             ...,\n",
       "             [ 1.0142e+00,  9.2797e-02, -5.5357e-01,  ..., -1.7186e-01,\n",
       "               1.9816e-03,  3.8424e-01],\n",
       "             [ 7.1393e-01, -1.6414e-02, -9.5312e-02,  ...,  2.8005e-01,\n",
       "               3.4736e-01,  1.0904e+00],\n",
       "             [-4.8755e-01, -8.4580e-01, -1.6416e+00,  ..., -4.1972e-01,\n",
       "               9.1594e-03,  2.0602e-01]],\n",
       "   \n",
       "            [[ 1.6993e+00,  1.4268e+00, -2.3628e-01,  ...,  7.1623e-01,\n",
       "              -5.3518e-01,  2.8722e-01],\n",
       "             [ 4.0055e-01,  8.0831e-01, -3.6808e-02,  ...,  4.7556e-01,\n",
       "              -4.6841e-01,  1.3664e+00],\n",
       "             [-7.3669e-01,  5.7053e-01, -4.4190e-01,  ..., -3.1856e-01,\n",
       "              -1.3420e+00,  1.0085e+00],\n",
       "             ...,\n",
       "             [-8.4266e-01, -3.3048e-01,  6.1870e-01,  ..., -1.0854e+00,\n",
       "               1.5524e+00, -2.2783e-01],\n",
       "             [ 5.2243e-02, -1.0934e+00, -5.7714e-02,  ..., -3.6486e-01,\n",
       "              -1.4302e-01, -1.0258e+00],\n",
       "             [-1.8034e-01,  3.5397e-01, -8.9258e-01,  ..., -1.8654e-02,\n",
       "              -2.8380e-01,  9.3827e-01]],\n",
       "   \n",
       "            [[ 3.4368e-01, -8.2835e-01,  1.4998e+00,  ..., -1.9085e-01,\n",
       "               4.3401e-01, -4.8347e-01],\n",
       "             [-9.0797e-02,  2.6654e-01,  4.1414e-02,  ..., -2.8379e-01,\n",
       "              -1.3817e+00, -6.7595e-01],\n",
       "             [ 3.1368e-01, -3.7680e-01,  5.0597e-01,  ..., -8.4706e-01,\n",
       "              -2.6706e-01,  4.6136e-01],\n",
       "             ...,\n",
       "             [ 1.6981e-01,  1.0737e+00, -7.2575e-01,  ...,  8.2870e-01,\n",
       "               1.9792e-01, -1.2824e-01],\n",
       "             [ 3.1109e-01, -8.2359e-01,  1.4163e-01,  ...,  6.4499e-01,\n",
       "               4.1049e-01,  1.5566e+00],\n",
       "             [-2.3251e-01, -4.1203e-01, -1.3105e-01,  ..., -7.2429e-02,\n",
       "              -2.3172e-01, -4.5803e-01]],\n",
       "   \n",
       "            [[-3.1649e-01,  7.1304e-02,  2.3237e-01,  ...,  2.1043e-01,\n",
       "              -3.5426e-01,  4.7821e-01],\n",
       "             [ 3.9905e-01,  1.2112e+00, -8.8610e-01,  ...,  1.3792e+00,\n",
       "               1.6416e+00,  4.6022e-01],\n",
       "             [-7.3430e-01,  1.8120e-01,  4.4585e-01,  ..., -7.7749e-01,\n",
       "               4.6448e-01,  2.3645e-01],\n",
       "             ...,\n",
       "             [-3.1878e-01,  6.0648e-01,  6.8930e-01,  ..., -5.1422e-02,\n",
       "              -3.6786e-01,  7.2147e-01],\n",
       "             [ 1.2826e+00, -2.2837e-01, -3.3992e-01,  ...,  9.9968e-01,\n",
       "               7.2008e-01,  1.3592e+00],\n",
       "             [-4.9550e-01, -3.9511e-01, -1.4909e-01,  ...,  3.2481e-01,\n",
       "               7.9600e-01,  7.0864e-01]]]], grad_fn=<TransposeBackward0>)),))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the model\n",
    "input_dim = (2, 1024)\n",
    "example_arg = example_args_collection[0]\n",
    "model(*example_args_collection[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About the dump model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The wrapping boils down to:\n",
    "```\n",
    "- wrapper\n",
    "    - model\n",
    "        - LlamaDecoderLayer\n",
    "```\n",
    "The model then is:  \n",
    "![Llama model](llama.jpg)\n",
    "\n",
    "Which corresponds on the netron model to:  \n",
    "![Netron labeled](llama_netron_labeled.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step by step\n",
    "from transformers.cache_utils import Cache, DynamicCache, StaticCache\n",
    "from transformers.modeling_outputs import BaseModelOutputWithPast\n",
    "\n",
    "# args\n",
    "input_ids, attention_mask = example_arg\n",
    "_model = model.model\n",
    "\n",
    "# Forward\n",
    "output_attentions = _model.config.output_attentions\n",
    "output_hidden_states = _model.config.output_hidden_states\n",
    "use_cache = _model.config.use_cache\n",
    "return_dict = _model.config.use_return_dict\n",
    "inputs_embeds = _model.embed_tokens(input_ids)\n",
    "\n",
    "past_seen_tokens = 0\n",
    "if not isinstance(None, StaticCache):\n",
    "    past_key_values = DynamicCache.from_legacy_cache(None)\n",
    "    past_seen_tokens = past_key_values.get_seq_length()\n",
    "\n",
    "if isinstance(past_key_values, StaticCache):\n",
    "    raise ValueError(\"cache_position is a required argument when using StaticCache.\")\n",
    "cache_position = torch.arange(\n",
    "    past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n",
    ")\n",
    "\n",
    "position_ids = cache_position.unsqueeze(0)\n",
    "causal_mask = _model._update_causal_mask(attention_mask, inputs_embeds, cache_position, past_seen_tokens)\n",
    "\n",
    "# embed positions\n",
    "hidden_states = inputs_embeds\n",
    "\n",
    "# decoder layers\n",
    "all_hidden_states = None\n",
    "all_self_attns = None\n",
    "next_decoder_cache = None\n",
    "\n",
    "decoder_layer = _model.layers[0]\n",
    "\n",
    "# forward from decoder layer\n",
    "attention_mask=causal_mask\n",
    "position_ids=position_ids\n",
    "past_key_value=past_key_values\n",
    "output_attentions=output_attentions\n",
    "use_cache=use_cache\n",
    "cache_position=cache_position\n",
    "\n",
    "residual = hidden_states\n",
    "hidden_states = decoder_layer.input_layernorm(hidden_states) # hidden_states is x\n",
    "\n",
    "# Self Attention\n",
    "hidden_states, self_attn_weights, present_key_value = decoder_layer.self_attn(\n",
    "    hidden_states=hidden_states,\n",
    "    attention_mask=attention_mask,\n",
    "    position_ids=position_ids,\n",
    "    past_key_value=past_key_value,\n",
    "    output_attentions=output_attentions,\n",
    "    use_cache=use_cache,\n",
    "    cache_position=cache_position\n",
    ")\n",
    "hidden_states = residual + hidden_states\n",
    "\n",
    "# Fully Connected\n",
    "residual = hidden_states\n",
    "hidden_states = decoder_layer.post_attention_layernorm(hidden_states)\n",
    "mlp_input = hidden_states # for mlp fuzed kernel\n",
    "hidden_states = decoder_layer.mlp(hidden_states)\n",
    "hidden_states = residual + hidden_states\n",
    "\n",
    "outputs = (hidden_states,)\n",
    "\n",
    "# if output_attentions:\n",
    "#     outputs += (self_attn_weights,)\n",
    "\n",
    "outputs += (present_key_value,)\n",
    "\n",
    "layer_outputs = outputs\n",
    "# end of forward from decoder layer\n",
    "\n",
    "hidden_states = layer_outputs[0]\n",
    "\n",
    "next_decoder_cache = layer_outputs[2 if output_attentions else 1]\n",
    "\n",
    "hidden_states = _model.norm(hidden_states)\n",
    "\n",
    "# add hidden states from the last decoder layer\n",
    "if output_hidden_states:\n",
    "    all_hidden_states += (hidden_states,)\n",
    "\n",
    "next_cache = None\n",
    "if use_cache:\n",
    "    next_cache = (\n",
    "        next_decoder_cache.to_legacy_cache() if isinstance(next_decoder_cache, Cache) else next_decoder_cache\n",
    "    )\n",
    "\n",
    "_return = BaseModelOutputWithPast(\n",
    "last_hidden_state=hidden_states,\n",
    "past_key_values=next_cache,\n",
    "hidden_states=all_hidden_states,\n",
    "attentions=all_self_attns,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mlp fuzed kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate the MLP\n",
    "mlp = decoder_layer.mlp\n",
    "torch.onnx.export(\n",
    "    mlp,\n",
    "    (mlp_input,),\n",
    "    \"mlp.onnx\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `mlp` module actually correspond to a gated multi-layer perceptron.  \n",
    "See https://arxiv.org/pdf/2002.05202 as a reference (we later call the Swish function SiLU). \n",
    "The gated linear unit returns:  \n",
    "`self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))`, that is to say:\n",
    "$$(\\sigma(X \\times G^T) \\odot (X \\times U^T)) \\times D^T$$\n",
    "With the shapes:\n",
    "- $X: n,d$\n",
    "- $U: \\tilde{d},d$\n",
    "- $G: \\tilde{d},d$\n",
    "- $D: d,\\tilde{d}$  \n",
    "\n",
    "$\\sigma$ corresponds to the $SiLU$ activation function: $\\sigma(x)=x*sigmoid(x)$\n",
    "\n",
    "![MLP netron labeled](mlp_netron_labeled.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "hidden_size = _model.config.hidden_size\n",
    "intermediate_size = _model.config.intermediate_size\n",
    "mlp_input_shape = mlp_input.shape\n",
    "G = mlp.gate_proj.weight\n",
    "U = mlp.up_proj.weight\n",
    "D = mlp.down_proj.weight\n",
    "\n",
    "# Forward\n",
    "X = mlp_input\n",
    "output = mlp(mlp_input)\n",
    "act_fn = torch.nn.SiLU()\n",
    "_ = (act_fn(X@G.T)*(X@U.T)) @ D.T\n",
    "assert torch.allclose(output, _)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export\n",
    "torch.save(mlp,\"mlp.pt\")\n",
    "torch.save(mlp_input,\"mlp_input.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Triton kernel\n",
    "https://developer.nvidia.com/blog/cuda-refresher-cuda-programming-model/  \n",
    "https://triton-lang.org/main/getting-started/tutorials/01-vector-add.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I start by exploring my GPU characteristics, using CUDA demo suite `deviceQuery` (`Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.1\\extras\\demo_suite`):\n",
    "```\n",
    "Device 0: \"NVIDIA GeForce GTX 950M\"\n",
    "  CUDA Driver Version / Runtime Version          12.1 / 12.1\n",
    "  CUDA Capability Major/Minor version number:    5.0\n",
    "  Total amount of global memory:                 2048 MBytes (2147352576 bytes)\n",
    "  ( 5) Multiprocessors, (128) CUDA Cores/MP:     640 CUDA Cores\n",
    "  GPU Max Clock rate:                            1124 MHz (1.12 GHz)\n",
    "  Memory Clock rate:                             1001 Mhz\n",
    "  Memory Bus Width:                              128-bit\n",
    "  L2 Cache Size:                                 2097152 bytes\n",
    "  Maximum Texture Dimension Size (x,y,z)         1D=(65536), 2D=(65536, 65536), 3D=(4096, 4096, 4096)\n",
    "  Maximum Layered 1D Texture Size, (num) layers  1D=(16384), 2048 layers\n",
    "  Maximum Layered 2D Texture Size, (num) layers  2D=(16384, 16384), 2048 layers\n",
    "  Total amount of constant memory:               zu bytes\n",
    "  Total amount of shared memory per block:       zu bytes\n",
    "  Total number of registers available per block: 65536\n",
    "  Warp size:                                     32\n",
    "  Maximum number of threads per multiprocessor:  2048\n",
    "  Maximum number of threads per block:           1024\n",
    "  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)\n",
    "  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)\n",
    "  Maximum memory pitch:                          zu bytes\n",
    "  Texture alignment:                             zu bytes\n",
    "  Concurrent copy and kernel execution:          Yes with 4 copy engine(s)\n",
    "  Run time limit on kernels:                     Yes\n",
    "  Integrated GPU sharing Host Memory:            No\n",
    "  Support host page-locked memory mapping:       Yes\n",
    "  Alignment requirement for Surfaces:            Yes\n",
    "  Device has ECC support:                        Disabled\n",
    "  CUDA Device Driver Mode (TCC or WDDM):         WDDM (Windows Display Driver Model)\n",
    "  Device supports Unified Addressing (UVA):      Yes\n",
    "  Device supports Compute Preemption:            No\n",
    "  Supports Cooperative Kernel Launch:            No\n",
    "  Supports MultiDevice Co-op Kernel Launch:      No\n",
    "  Device PCI Domain ID / Bus ID / location ID:   0 / 1 / 0\n",
    "  Compute Mode:\n",
    "     < Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) >\n",
    "```\n",
    "\n",
    "I need to compute:\n",
    "$$\n",
    "\\begin{cases}\n",
    "    Z =  & X \\times G^T\\\\\n",
    "    \\tilde{Z} = & Z \\odot sigmoid(Z) \\odot (X\\times U^T)\\\\\n",
    "    ... = & \\tilde{Z} \\times D^T\n",
    "  \\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I propose the following parallelization scheme:\n",
    "- Compute $Z$ and $X \\times U^T$ in parallel\n",
    "- Compute $sigmoid(Z)$\n",
    "- Compute the triple element wise multiplication for $\\tilde{Z}$\n",
    "- Compute $\\tilde{Z} \\times D^T$  \n",
    "\n",
    "The first step is natively implemented when using\n",
    "$X \\times ( \n",
    "    \\begin{array}{c}\n",
    "    G^T \\\\\n",
    "    U^T \\\\\n",
    "    \\end{array}\n",
    "    )$\n",
    "and masks to retrieve results.\n",
    "The novelty is step 3, that is to say a kernel for triple elementwise matrix multiplication.\n",
    "Performance improvements from other steps will not come from parallelization but from cach usage optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a naive implementation, I get the following performance:  \n",
    "![Naive triton kernel](naive_triton_kernel.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
