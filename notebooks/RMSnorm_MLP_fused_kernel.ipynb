{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available! You can use GPU acceleration.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA (GPU support) is available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available! You can use GPU acceleration.\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Using CPU instead.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers\n",
    "#!pip install matplotlib\n",
    "#!pip install pandas "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FUSED MLP kernel with RMSnorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Triton kernel\n",
    "https://developer.nvidia.com/blog/cuda-refresher-cuda-programming-model/  \n",
    "https://triton-lang.org/main/getting-started/tutorials/01-vector-add.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I start by exploring my GPU characteristics, using CUDA demo suite `deviceQuery` (`Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.1\\extras\\demo_suite`):\n",
    "```\n",
    "Device 0: \"NVIDIA GeForce GTX 950M\"\n",
    "  CUDA Driver Version / Runtime Version          12.1 / 12.1\n",
    "  CUDA Capability Major/Minor version number:    5.0\n",
    "  Total amount of global memory:                 2048 MBytes (2147352576 bytes)\n",
    "  ( 5) Multiprocessors, (128) CUDA Cores/MP:     640 CUDA Cores\n",
    "  GPU Max Clock rate:                            1124 MHz (1.12 GHz)\n",
    "  Memory Clock rate:                             1001 Mhz\n",
    "  Memory Bus Width:                              128-bit\n",
    "  L2 Cache Size:                                 2097152 bytes\n",
    "  Maximum Texture Dimension Size (x,y,z)         1D=(65536), 2D=(65536, 65536), 3D=(4096, 4096, 4096)\n",
    "  Maximum Layered 1D Texture Size, (num) layers  1D=(16384), 2048 layers\n",
    "  Maximum Layered 2D Texture Size, (num) layers  2D=(16384, 16384), 2048 layers\n",
    "  Total amount of constant memory:               zu bytes\n",
    "  Total amount of shared memory per block:       zu bytes\n",
    "  Total number of registers available per block: 65536\n",
    "  Warp size:                                     32\n",
    "  Maximum number of threads per multiprocessor:  2048\n",
    "  Maximum number of threads per block:           1024\n",
    "  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)\n",
    "  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)\n",
    "  Maximum memory pitch:                          zu bytes\n",
    "  Texture alignment:                             zu bytes\n",
    "  Concurrent copy and kernel execution:          Yes with 4 copy engine(s)\n",
    "  Run time limit on kernels:                     Yes\n",
    "  Integrated GPU sharing Host Memory:            No\n",
    "  Support host page-locked memory mapping:       Yes\n",
    "  Alignment requirement for Surfaces:            Yes\n",
    "  Device has ECC support:                        Disabled\n",
    "  CUDA Device Driver Mode (TCC or WDDM):         WDDM (Windows Display Driver Model)\n",
    "  Device supports Unified Addressing (UVA):      Yes\n",
    "  Device supports Compute Preemption:            No\n",
    "  Supports Cooperative Kernel Launch:            No\n",
    "  Supports MultiDevice Co-op Kernel Launch:      No\n",
    "  Device PCI Domain ID / Bus ID / location ID:   0 / 1 / 0\n",
    "  Compute Mode:\n",
    "     < Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) >\n",
    "```\n",
    "\n",
    "I need to compute:\n",
    "\n",
    "![RMSNorm](RMSNorm_arxiv.png)\n",
    "\n",
    "\n",
    "See https://arxiv.org/abs/1910.07467 as a reference. \n",
    "\n",
    "and then on X_normalized\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "    Z =  & X \\times G^T\\\\\n",
    "    \\tilde{Z} = & Z \\odot sigmoid(Z) \\odot (X\\times U^T)\\\\\n",
    "    ... = & \\tilde{Z} \\times D^T\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fused RMSnorm + MLP kernel into one "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "l'idée est de combiner plusieurs étapes de calcul en une seule boucle optimisée, réduisant ainsi les redondances et améliorant l'efficacité globale du modèle.\n",
    "\n",
    "\n",
    "L'objectif est d'optimiser la section feed-forward du modèle LLAMA pour améliorer les performances. Voici les étapes et l'idée générale de ce que nous voulons coder :\n",
    "\n",
    "1. **Normalisation RMS (RMSNorm) :** Appliquer une normalisation RMS à l'entrée une seule fois et réutiliser cette sortie normalisée pour les opérations suivantes. Actuellement, `x_norm` est calculé une fois puis utilisé dans deux multiplications matricielles séparées.\n",
    "\n",
    "2. **Multiplications Matricielles en Chaîne :** Enchaîner les deux multiplications matricielles dans une seule boucle pour réduire les accès mémoire et améliorer l'efficacité. La première multiplication matricielle est suivie par une opération `silu` (qui combine une multiplication et une activation sigmoïde), puis une deuxième multiplication matricielle est effectuée.\n",
    "\n",
    "3. **Fusion des Opérations :** Fusionner l'opération `silu` avec la sortie de la première multiplication matricielle pour éviter des opérations redondantes. Cette fusion implique de combiner les opérations élémentaires de manière à ce qu'elles soient effectuées en une seule étape.\n",
    "\n",
    "4. **Optimisation des Statistiques RMS :** Calculer et appliquer les statistiques RMS dans la même boucle que les multiplications matricielles, réduisant ainsi le nombre de passes nécessaires sur les données.\n",
    "\n",
    "5. **Amélioration des Performances :** En rationalisant ces opérations, le nouveau noyau sera plus rapide que l'implémentation actuelle en PyTorch. Les optimisations visent à réduire le temps d'exécution en minimisant les opérations redondantes et en améliorant l'utilisation des ressources.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Triton and Torch match in the range of 1e-1\n",
      "rms matmul silu mul pytorch 0.69377601146698\n",
      "rms matmul silu mul triton fp8 0.5414638519287109\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "from pytorch_kernel import rms_norm_pytorch\n",
    "from utils import f16_to_f8, f8_to_f16\n",
    "\n",
    "\n",
    "@triton.jit\n",
    "def ff_llama(\n",
    "    a_ptr, w1_ptr, w3_ptr, out_ptr, rms_w_ptr,\n",
    "    M, N, K,\n",
    "    stride_am, stride_ak,\n",
    "    stride_w1k, stride_w1n,\n",
    "    stride_w3k, stride_w3n,\n",
    "    stride_outm, stride_outn,\n",
    "    stride_rms_w,\n",
    "    USE_FP8: tl.constexpr,\n",
    "    EPS: tl.constexpr,\n",
    "    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n",
    "):\n",
    "    \"\"\"\n",
    "    w1 and w3 are weights (linear layers)\n",
    "    F.silu(w1(x)) * w3(x)\n",
    "    \"\"\"\n",
    "    pid = tl.program_id(axis=0)\n",
    "    pid_m = pid // tl.cdiv(N, BLOCK_SIZE_N)\n",
    "    pid_n = pid % tl.cdiv(N, BLOCK_SIZE_N)\n",
    "\n",
    "    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n",
    "    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n",
    "    offs_k = tl.arange(0, BLOCK_SIZE_K)\n",
    "    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n",
    "    w1_ptrs = w1_ptr + (offs_k[:, None] * stride_w1k + offs_bn[None, :] * stride_w1n)\n",
    "    w3_ptrs = w3_ptr + (offs_k[:, None] * stride_w3k + offs_bn[None, :] * stride_w3n)\n",
    "    acc1 = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n",
    "    acc2 = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n",
    "\n",
    "    rms_w_ptrs = rms_w_ptr + tl.arange(0, BLOCK_SIZE_K)[None, :] * stride_rms_w\n",
    "    a_sum = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_K), dtype=tl.float32)\n",
    "    for _ in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n",
    "        a = tl.load(a_ptrs)\n",
    "        a_sum += tl.math.pow(a.to(tl.float32), 2)\n",
    "        rms_w = tl.load(rms_w_ptrs)\n",
    "        if USE_FP8:\n",
    "            rms_w = rms_w.to(tl.float8e5, bitcast=True)\n",
    "            rms_w = rms_w.to(tl.float16)\n",
    "        a = a * rms_w\n",
    "        b = tl.load(w1_ptrs)\n",
    "        if USE_FP8:\n",
    "            b = b.to(tl.float8e5, bitcast=True)\n",
    "            b = b.to(tl.float32)\n",
    "            b = b.to(tl.float16)\n",
    "        acc1 += tl.dot(a, b)\n",
    "        c = tl.load(w3_ptrs)\n",
    "        if USE_FP8:\n",
    "            c = c.to(tl.float8e5, bitcast=True)\n",
    "            c = c.to(tl.float32)\n",
    "            c = c.to(tl.float16)\n",
    "        acc2 += tl.dot(a, c)\n",
    "\n",
    "        a_ptrs += BLOCK_SIZE_K * stride_ak\n",
    "        w1_ptrs += BLOCK_SIZE_K * stride_w1k\n",
    "        w3_ptrs += BLOCK_SIZE_K * stride_w3k\n",
    "\n",
    "        rms_w_ptrs += BLOCK_SIZE_K * stride_rms_w\n",
    "\n",
    "    a_mean = tl.sum(a_sum, axis=1) / K + EPS\n",
    "    a_norm = tl.math.rsqrt(a_mean)\n",
    "    acc1 = acc1 * a_norm[:, None]\n",
    "    acc2 = acc2 * a_norm[:, None]\n",
    "    accumulator = (acc1 * tl.sigmoid(acc1)) * acc2\n",
    "\n",
    "    offs_outm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n",
    "    offs_outn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n",
    "    out_ptrs = out_ptr + (stride_outm * offs_outm[:, None] + stride_outn * offs_outn[None, :])\n",
    "    out_mask = (offs_outm[:, None] < M) & (offs_outn[None, :] < N)\n",
    "    tl.store(out_ptrs, accumulator, mask=out_mask)\n",
    "\n",
    "\n",
    "def kernel_ff(x: torch.Tensor, w1: torch.Tensor, w3: torch.Tensor, rms_w: torch.Tensor) -> torch.Tensor:\n",
    "    assert x.dtype == torch.float16\n",
    "    assert w1.dtype == w3.dtype == rms_w.dtype\n",
    "    assert w1.dtype in [torch.int8, torch.float16]\n",
    "    assert w1.shape == w3.shape\n",
    "\n",
    "    w1_t = w1.t()\n",
    "    w3_t = w3.t()\n",
    "\n",
    "    batch, seq_len, dim = x.shape\n",
    "    M, K = batch * seq_len, dim\n",
    "\n",
    "    N = w1_t.shape[1]\n",
    "    assert K == w1_t.shape[0]\n",
    "    assert w1_t.shape == w3_t.shape\n",
    "    x_reshape = x.reshape(M, K)\n",
    "    out = torch.empty((M, N), dtype=x.dtype, device=x.device)\n",
    "    grid = lambda META: (triton.cdiv(META[\"M\"], META[\"BLOCK_SIZE_M\"]) * triton.cdiv(META[\"N\"], META[\"BLOCK_SIZE_N\"]),)\n",
    "    ff_llama[grid](\n",
    "        x_reshape, w1_t, w3_t, out, rms_w,\n",
    "        M, N, K,\n",
    "        *x_reshape.stride(),\n",
    "        *w1_t.stride(),\n",
    "        *w3_t.stride(),\n",
    "        *out.stride(),\n",
    "        *rms_w.stride(),\n",
    "        USE_FP8=w1_t.dtype != torch.float16,\n",
    "        EPS=1e-6,\n",
    "        BLOCK_SIZE_M=16, BLOCK_SIZE_N=16, BLOCK_SIZE_K=64,\n",
    "        num_stages=2, num_warps=4\n",
    "    )\n",
    "    out = out.view(batch, seq_len, -1)\n",
    "    return out\n",
    "\n",
    "\n",
    "x = torch.randn([1, 16, 4096], dtype=torch.float16, device=\"cuda\")\n",
    "# weights tends to be very small values\n",
    "rms_w = torch.randn([4096], dtype=torch.float16, device=\"cuda\") * 0.2\n",
    "w1_w = torch.randn([11008, 4096], dtype=torch.float16, device=\"cuda\") * 0.2\n",
    "w3_w = torch.randn([11008, 4096], dtype=torch.float16, device=\"cuda\") * 0.2\n",
    "\n",
    "\n",
    "x_norm_p = rms_norm_pytorch(x, rms_w, eps=1e-6)\n",
    "w1_p = x_norm_p @ w1_w.t()\n",
    "w1_silu_p = torch.nn.functional.silu(w1_p)\n",
    "w3_p = x_norm_p @ w3_w.t()\n",
    "\n",
    "\n",
    "def ff_pytorch(x: torch.Tensor, w1: torch.Tensor, w3: torch.Tensor, rms_w: torch.Tensor) -> torch.Tensor:\n",
    "    x_norm = rms_norm_pytorch(x, rms_w, eps=1e-6)\n",
    "    a = torch.nn.functional.silu(torch.matmul(x_norm, w1.t()))\n",
    "    b = torch.matmul(x_norm, w3.t())\n",
    "    return a * b\n",
    "\n",
    "\n",
    "output_triton = kernel_ff(x=x, w1=w1_w, w3=w3_w, rms_w=rms_w)\n",
    "output_pytorch = ff_pytorch(x=x, w1=w1_w, w3=w3_w, rms_w=rms_w)\n",
    "\n",
    "if torch.allclose(output_triton, w1_silu_p * w3_p, atol=1e-1):\n",
    "    print(\"✅ Triton and Torch match in the range of 1e-1\")\n",
    "else:\n",
    "    print(\"❌ Triton and Torch differ in the range of 1e-1\")\n",
    "\n",
    "\n",
    "#print(\"rms matmul silu mul triton\", triton.testing.do_bench(lambda: kernel_ff(x=x, w1=w1_w, w3=w3_w, rms_w=rms_w)))\n",
    "print(\"rms matmul silu mul pytorch\", triton.testing.do_bench(lambda: ff_pytorch(x=x, w1=w1_w, w3=w3_w, rms_w=rms_w)))\n",
    "\n",
    "w1_w_fp8 = f16_to_f8(w1_w, dtypes=tl.float8e5)\n",
    "w3_w_fp8 = f16_to_f8(w3_w, dtypes=tl.float8e5)\n",
    "rms_w_fp8 = f16_to_f8(rms_w, dtypes=tl.float8e5)\n",
    "\n",
    "out_fp8 = kernel_ff(x=x, w1=w1_w_fp8, w3=w3_w_fp8, rms_w=rms_w_fp8)\n",
    "# on very large tensors, it is expected that the error is large, we just check it is not crazy large\n",
    "assert torch.allclose(out_fp8, w1_silu_p * w3_p, atol=10)\n",
    "\n",
    "print(\"rms matmul silu mul triton fp8\", triton.testing.do_bench(lambda: kernel_ff(x=x, w1=w1_w_fp8, w3=w3_w_fp8, rms_w=rms_w_fp8)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " `triton.testing.do_bench` fournit des informations détaillées sur la performance de la fonction `kernel_ff` en termes de temps d'exécution sur le GPU, ce qui est crucial pour évaluer et optimiser les performances des noyaux CUDA."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
